{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3c3ec5-b1e9-4a39-b350-fe899908bfdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: S&P_500_closing_price_interpolated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: Gold_closing_price_interpolated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: ICE_U.S._Dollar_Index_closing_price_interpolated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: WTI_Crude_Oil_closing_price_interpolated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File created: VIX_index_closing_price_interpolated.csv\n",
      "Merged data saved to combined_data.csv.\n",
      "           Date      S&P 500         Gold  ICE U.S. Dollar Index  \\\n",
      "0    2000-08-30  1502.589966   273.899994             112.139999   \n",
      "1    2000-08-31  1517.680054   278.299988             112.599998   \n",
      "2    2000-09-01  1520.770020   277.000000             111.419998   \n",
      "3    2000-09-02  1517.347504   276.699997             111.667500   \n",
      "4    2000-09-03  1513.924988   276.399994             111.915001   \n",
      "...         ...          ...          ...                    ...   \n",
      "8818 2024-10-21  5853.979980  2723.100098             104.010002   \n",
      "8819 2024-10-22  5851.200195  2744.199951             104.080002   \n",
      "8820 2024-10-23  5797.419922  2714.399902             104.430000   \n",
      "8821 2024-10-24  5809.859863  2734.899902             104.059998   \n",
      "8822 2024-10-25  5808.120117  2740.899902             104.260002   \n",
      "\n",
      "      WTI Crude Oil  VIX index  \n",
      "0         33.400002  17.690001  \n",
      "1         33.099998  16.840000  \n",
      "2         33.380001  17.530001  \n",
      "3         33.485001  18.102500  \n",
      "4         33.590000  18.675000  \n",
      "...             ...        ...  \n",
      "8818      70.559998  18.370001  \n",
      "8819      72.089996  18.200001  \n",
      "8820      70.769997  19.240000  \n",
      "8821      70.190002  19.080000  \n",
      "8822      71.779999  20.330000  \n",
      "\n",
      "[8823 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define a dictionary with indices and their corresponding ticker symbols\n",
    "indices = {\n",
    "    'S&P 500': '^GSPC',\n",
    "    'Gold': 'GC=F',\n",
    "    'ICE U.S. Dollar Index': '^NYICDX',\n",
    "    'WTI Crude Oil': 'CL=F',\n",
    "    'VIX index': '^VIX'\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store individual DataFrames\n",
    "dataframes = {}\n",
    "\n",
    "# Loop over each index to download data, process it, save as CSV, and prepare for merging\n",
    "for index_name, ticker in indices.items():\n",
    "    # Define the filename\n",
    "    filename = f\"{index_name.replace(' ', '_')}_closing_price_interpolated.csv\"\n",
    "    \n",
    "    # Download data for the index\n",
    "    try:\n",
    "        data = yf.download(ticker, start='1927-12-30', end='2024-10-28')[['Close']].reset_index()\n",
    "        data.columns = ['Date', 'Close_Price']\n",
    "\n",
    "        # Convert date to datetime, set as index, and resample\n",
    "        data = data.assign(Date=pd.to_datetime(data['Date'])).set_index('Date').resample('D').mean()\n",
    "\n",
    "        # Interpolate missing values\n",
    "        data['Close_Price'] = data['Close_Price'].interpolate(method='linear')\n",
    "\n",
    "        # Reset index and save to CSV\n",
    "        data.reset_index().to_csv(filename, index=False)\n",
    "        print(f\"File created: {filename}\")\n",
    "\n",
    "        # Rename the \"Close_Price\" column to the index name and store in dictionary\n",
    "        df = data.reset_index()\n",
    "        df = df.rename(columns={'Close_Price': index_name})\n",
    "        dataframes[index_name] = df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {index_name}: {e}\")\n",
    "\n",
    "# Check if we have enough DataFrames to merge\n",
    "if not dataframes:\n",
    "    print(\"No data was successfully processed. Exiting.\")\n",
    "else:\n",
    "    # Merge all DataFrames on the \"Date\" column\n",
    "    merged_df = None\n",
    "    for index_name, df in dataframes.items():\n",
    "        if merged_df is None:\n",
    "            merged_df = df  # Start with the first DataFrame\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='Date', how='inner')  # Merge subsequent DataFrames\n",
    "    \n",
    "    # Save the merged DataFrame to a new CSV file\n",
    "    merged_filename = \"combined_data.csv\"\n",
    "    merged_df.to_csv(merged_filename, index=False)\n",
    "    print(f\"Merged data saved to {merged_filename}.\")\n",
    "\n",
    "print(merged_df)  # Display the last 10 rows of the merged DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79680fa1-6a30-46be-9061-a3fa203fd374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7582d46-85a5-4eba-994b-33fb5397ba5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/network/anaconda3/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:241: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date      S&P 500         Gold  ICE U.S. Dollar Index  \\\n",
      "0    2016-03-01  1978.349976  1230.300049              98.349998   \n",
      "1    2016-03-02  1986.449951  1241.099976              98.209999   \n",
      "2    2016-03-03  1993.400024  1257.400024              97.570000   \n",
      "3    2016-03-04  1999.989990  1269.900024              97.339996   \n",
      "4    2016-03-05  2000.579997  1267.666667              97.249997   \n",
      "...         ...          ...          ...                    ...   \n",
      "3156 2024-10-21  5853.979980  2723.100098             104.010002   \n",
      "3157 2024-10-22  5851.200195  2744.199951             104.080002   \n",
      "3158 2024-10-23  5797.419922  2714.399902             104.430000   \n",
      "3159 2024-10-24  5809.859863  2734.899902             104.059998   \n",
      "3160 2024-10-25  5808.120117  2740.899902             104.260002   \n",
      "\n",
      "      WTI Crude Oil  VIX index  Rate (%)  \n",
      "0         34.400002  17.700001      0.36  \n",
      "1         34.660000  17.090000      0.37  \n",
      "2         34.570000  16.700001      0.37  \n",
      "3         35.919998  16.860001      0.36  \n",
      "4         36.579999  17.023334      0.36  \n",
      "...             ...        ...       ...  \n",
      "3156      70.559998  18.370001      4.83  \n",
      "3157      72.089996  18.200001      4.83  \n",
      "3158      70.769997  19.240000      4.83  \n",
      "3159      70.190002  19.080000      4.83  \n",
      "3160      71.779999  20.330000      4.83  \n",
      "\n",
      "[3161 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Excel file to download\n",
    "url = \"https://markets.newyorkfed.org/read?startDt=2016-03-01&endDt=2024-10-25&eventCodes=500&productCode=50&sort=postDt:-1,eventCode:1&format=xlsx\"\n",
    "\n",
    "# Send a GET request to download the Excel file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Specify the local file path to save the downloaded Excel file\n",
    "file_path = \"../EFFR_data.xlsx\"\n",
    "\n",
    "# Save the Excel file locally\n",
    "with open(file_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Read the Excel file using pandas with the 'xlsxwriter' engine\n",
    "df_EFFR = pd.read_excel(file_path)\n",
    "\n",
    "# Convert 'Effective Date' column in df_EFFR to datetime\n",
    "df_EFFR['Effective Date'] = pd.to_datetime(df_EFFR['Effective Date'])\n",
    "\n",
    "# Select only the 'Effective Date' and 'Rate (%)' columns from df_EFFR\n",
    "df_EFFR = df_EFFR[['Effective Date', 'Rate (%)']]\n",
    "\n",
    "# Set the 'Effective Date' column as the index of df_EFFR\n",
    "df_EFFR.set_index('Effective Date', inplace=True)\n",
    "\n",
    "# Merge the \"Rate (%)\" column from df_EFFR to merged_df_step1 based on the date column\n",
    "merged_df = merged_df.merge(df_EFFR, how='left', left_on='Date', right_index=True)\n",
    "\n",
    "# Remove rows with NA values in the 'Rate (%)' column\n",
    "merged_df = merged_df.dropna(subset=['Rate (%)'])\n",
    "\n",
    "# Interpolate all columns in merged_df to fill in missing values\n",
    "merged_df = merged_df.set_index('Date')\n",
    "merged_df = merged_df.resample('D').interpolate(method='linear')\n",
    "merged_df = merged_df.reset_index()\n",
    "merged_df = merged_df.rename(columns={'index': 'Date'})\n",
    "\n",
    "# Print the first few rows to verify the merge\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d2b5554-985c-4de2-8421-fe30ff042b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            S&P 500_Binary  Gold_Binary  ICE U.S. Dollar Index_Binary  \\\n",
      "Date                                                                    \n",
      "2016-03-01               0            0                             0   \n",
      "2016-03-02               0            0                             0   \n",
      "2016-03-03               0            0                             0   \n",
      "2016-03-04               0            0                             0   \n",
      "2016-03-05               0            0                             0   \n",
      "2016-03-06               1            1                             0   \n",
      "2016-03-07               1            1                             0   \n",
      "2016-03-08               0            0                             0   \n",
      "2016-03-09               0            0                             0   \n",
      "2016-03-10               0            0                             0   \n",
      "\n",
      "            WTI Crude Oil_Binary  VIX index_Binary  Rate (%)_Binary  \n",
      "Date                                                                 \n",
      "2016-03-01                     0                 0                0  \n",
      "2016-03-02                     0                 0                0  \n",
      "2016-03-03                     0                 0                0  \n",
      "2016-03-04                     0                 0                0  \n",
      "2016-03-05                     0                 0                0  \n",
      "2016-03-06                     1                 0                0  \n",
      "2016-03-07                     1                 1                0  \n",
      "2016-03-08                     1                 1                0  \n",
      "2016-03-09                     1                 1                0  \n",
      "2016-03-10                     1                 1                0  \n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "from darts import TimeSeries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate moving averages\n",
    "def calculate_moving_averages(df, window):\n",
    "    return df.rolling(window=window).mean()\n",
    "\n",
    "# Calculate 3-day and 6-day moving averages for all columns except 'Date'\n",
    "for col in merged_df.columns:\n",
    "    if col not in ['Date']:\n",
    "        merged_df[col + '_3D_MA'] = calculate_moving_averages(merged_df[col], window=3)\n",
    "        merged_df[col + '_6D_MA'] = calculate_moving_averages(merged_df[col], window=6)\n",
    "\n",
    "# Convert 'Date' column to datetime type and set it as index\n",
    "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
    "merged_df.set_index('Date', inplace=True)\n",
    "\n",
    "# Convert moving averages to binary variables\n",
    "for col in merged_df.columns:\n",
    "    if '_3D_MA' in col:\n",
    "        base_col = col.replace('_3D_MA', '')\n",
    "        merged_df[base_col + '_Binary'] = np.where(merged_df[col] > merged_df[base_col + '_6D_MA'], 1, 0)\n",
    "\n",
    "# Remove original columns and moving average columns\n",
    "columns_to_drop = [col for col in merged_df.columns if not col.endswith('_Binary')]\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3be04dd-06c1-431f-9df5-12b80c422624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for fold starting at index 0: 0.5464\n",
      "Error for fold starting at index 50: 0.6384\n",
      "Error for fold starting at index 100: 0.9891\n",
      "Error for fold starting at index 150: 0.7204\n",
      "Error for fold starting at index 200: 0.7527\n",
      "Error for fold starting at index 250: 0.7268\n",
      "Error for fold starting at index 300: 0.6768\n",
      "Error for fold starting at index 350: 0.7101\n",
      "Error for fold starting at index 400: 0.7135\n",
      "Error for fold starting at index 450: 0.5476\n",
      "Error for fold starting at index 500: 0.7554\n",
      "Error for fold starting at index 550: 0.8087\n",
      "Error for fold starting at index 600: 0.6961\n",
      "Error for fold starting at index 650: 1.1394\n",
      "Error for fold starting at index 700: 0.7609\n",
      "Error for fold starting at index 750: 0.6272\n",
      "Error for fold starting at index 800: 0.7311\n",
      "Error for fold starting at index 850: 0.6230\n",
      "Error for fold starting at index 900: 0.6311\n",
      "Error for fold starting at index 950: 0.6330\n",
      "Error for fold starting at index 1000: 0.6902\n",
      "Error for fold starting at index 1050: 0.6317\n",
      "Error for fold starting at index 1100: 0.7108\n",
      "Error for fold starting at index 1150: 0.5477\n",
      "Error for fold starting at index 1200: 0.5964\n",
      "Error for fold starting at index 1250: 0.6133\n",
      "Error for fold starting at index 1300: 0.5789\n",
      "Error for fold starting at index 1350: 0.7286\n",
      "Error for fold starting at index 1400: 0.8294\n",
      "Error for fold starting at index 1450: 0.6860\n",
      "Error for fold starting at index 1500: 0.7104\n",
      "Error for fold starting at index 1550: 0.7105\n",
      "Error for fold starting at index 1600: 0.8674\n",
      "Error for fold starting at index 1650: 0.7243\n",
      "Error for fold starting at index 1700: 0.7435\n",
      "Error for fold starting at index 1750: 0.6923\n",
      "Error for fold starting at index 1800: 0.5553\n",
      "Error for fold starting at index 1850: 0.7426\n",
      "Error for fold starting at index 1900: 0.9018\n",
      "Error for fold starting at index 1950: 0.6170\n",
      "Error for fold starting at index 2000: 0.6447\n",
      "Error for fold starting at index 2050: 0.7034\n",
      "Error for fold starting at index 2100: 0.9038\n",
      "Error for fold starting at index 2150: 0.5782\n",
      "Error for fold starting at index 2200: 0.6067\n",
      "Error for fold starting at index 2250: 0.7316\n",
      "Error for fold starting at index 2300: 0.6136\n",
      "Error for fold starting at index 2350: 0.8531\n",
      "Error for fold starting at index 2400: 0.6315\n",
      "Error for fold starting at index 2450: 0.8061\n",
      "\n",
      "Best Error: 0.5464\n",
      "Best Parameters: {'C': 1, 'gamma': 'auto', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import log_loss, classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Data preprocessing\n",
    "# Assume merged_df is your DataFrame with the binary target variable and features\n",
    "# Assume 'S&P 500_Binary' is your target variable and the rest are features\n",
    "# Prepare the data\n",
    "X = merged_df.drop(columns=['S&P 500_Binary'])\n",
    "y = merged_df['S&P 500_Binary']\n",
    "\n",
    "# Step 2: Split the data\n",
    "train_size = 0.8\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "train_end = int(train_size * len(X))\n",
    "val_end = train_end + int(val_size * len(X))\n",
    "X_train_val, X_test = X[:val_end], X[val_end:]\n",
    "y_train_val, y_test = y[:val_end], y[val_end:]\n",
    "X_train, X_val = X_train_val[:train_end], X_train_val[train_end:]\n",
    "y_train, y_val = y_train_val[:train_end], y_train_val[train_end:]\n",
    "\n",
    "# Step 3: Define the walk-forward split window size and step size\n",
    "window_size = 50  # Fixed training window size of 50\n",
    "step_size = 50  # Step size of 50\n",
    "best_error = float('inf')  # Initialize best_error to track the lowest log loss\n",
    "best_params = None\n",
    "\n",
    "# Step 4: Walk-forward split (train on first 50, test on next 50, and slide forward)\n",
    "n_samples = len(X_train)\n",
    "for start in range(0, n_samples - window_size, step_size):\n",
    "    end = start + window_size\n",
    "    test_end = end + step_size  # 50 samples ahead for testing\n",
    "    if test_end > n_samples:\n",
    "        test_end = n_samples  # Ensure we don't go beyond the dataset\n",
    "\n",
    "    # Define training and testing data\n",
    "    X_train_fold, X_val_fold = X_train.iloc[start:end], X_val\n",
    "    y_train_fold, y_val_fold = y_train.iloc[start:end], y_val\n",
    "\n",
    "    # Check if y_val_fold contains both classes\n",
    "    if len(y_val_fold.unique()) < 2:\n",
    "        print(f\"Skipping fold starting at index {start} due to insufficient class labels in y_val_fold.\")\n",
    "        continue\n",
    "\n",
    "    # Step 5: Hyperparameter tuning using GridSearchCV\n",
    "    svm_model = SVC(probability=True)\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.04, 0.05, 0.06, 0.1, 1, 10, 20],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Kernel type\n",
    "        'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "    }\n",
    "    grid_search = GridSearchCV(svm_model, param_grid, cv=None, n_jobs=-1, verbose=0, scoring='neg_log_loss', error_score='raise')\n",
    "    try:\n",
    "        grid_search.fit(X_train_fold, y_train_fold)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred in fold starting at index {start}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Step 6: Train SVM model with best parameters from GridSearchCV\n",
    "    best_params_fold = grid_search.best_params_\n",
    "    svm_model.set_params(**best_params_fold)\n",
    "    svm_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Step 7: Evaluate the model on the validation set\n",
    "    y_val_pred_proba = svm_model.predict_proba(X_val_fold)[:, 1]\n",
    "    error = log_loss(y_val_fold, y_val_pred_proba)\n",
    "\n",
    "    # Track best error and parameters\n",
    "    if error < best_error:\n",
    "        best_error = error\n",
    "        best_params = best_params_fold  # Store the best parameters\n",
    "    print(f\"Error for fold starting at index {start}: {error:.4f}\")\n",
    "\n",
    "# Step 8: Final evaluation after all folds\n",
    "print(f\"\\nBest Error: {best_error:.4f}\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c929913-386a-42b9-8d1b-6dd1126c5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.81      0.66       107\n",
      "           1       0.88      0.68      0.76       210\n",
      "\n",
      "    accuracy                           0.72       317\n",
      "   macro avg       0.72      0.74      0.71       317\n",
      "weighted avg       0.77      0.72      0.73       317\n",
      "\n",
      "\n",
      "Final Confusion Matrix:\n",
      "[[ 87  20]\n",
      " [ 68 142]]\n",
      "\n",
      "Area Under Curve (AUC): 0.74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation on the testing set (using the best model)\n",
    "final_model = SVC(probability=True, **best_params)  # Train final model with the best parameters\n",
    "final_model.fit(X_train, y_train)\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "y_pred_proba_final = final_model.predict_proba(X_test)[:, 1]\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final))\n",
    "print(\"\\nFinal Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))\n",
    "\n",
    "# Calculate and print AUC\n",
    "auc_final = roc_auc_score(y_test, y_pred_proba_final)\n",
    "print(f\"\\nArea Under Curve (AUC): {auc_final:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ce0aa-392c-4724-9683-1857eca8f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcfbc7c-9355-4bed-bc8b-86def531ecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
